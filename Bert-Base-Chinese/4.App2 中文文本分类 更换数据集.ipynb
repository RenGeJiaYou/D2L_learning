{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset,load_from_disk\n",
    "from d2l import torch as d2l\n",
    "import re\n",
    "import jieba\n",
    "import time\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T02:24:48.496647200Z",
     "start_time": "2023-07-18T02:24:48.218341200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "导入本地 csv 文件,得到一个只含 train Dataset 的数据集"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/sjj/.cache/huggingface/datasets/csv/default-a6fb0b0e0bdd94cc/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3900239872d9457ebe3d4840684e0b6a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wb_data = load_dataset(\"csv\",data_files='../data/weibo_senti_100k.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T02:25:43.407506100Z",
     "start_time": "2023-07-18T02:25:39.259983900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['label', 'review'],\n    num_rows: 119988\n})"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wb_data['train']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-17T13:56:10.133057700Z",
     "start_time": "2023-07-17T13:56:10.094053400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 0. 清洗数据\n",
    "1. DataDict 对象调用 map() 时,不区分 train/test/validation 的 DataSet\n",
    "2. 试图开批量处理（设 batched=True）时调用 `re` 模块会报错，因为此时不允许回调函数 f 接收并返回`Dict[str, Any]`,而是`Dict[str, List]`,而 `re` 模块只能处理 `str` 而非 `list`。而在`Python`代码中直接写 for Loop 处理 `list` 效率不高\n",
    ">见（https://huggingface.co/docs/datasets/v2.13.1/en/package_reference/main_classes#datasets.DatasetDict.map.function）"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def f(d):\n",
    "    review = d['review']\n",
    "\n",
    "    # 清洗数据\n",
    "    # 删除‘//@用户名:’\n",
    "    pattern_1 = re.compile(r'//@.*?:')\n",
    "    s1 = re.sub(pattern_1, '', review)\n",
    "\n",
    "    # 删除‘@用户名 ’\n",
    "    pattern_2 = re.compile('@.*?\\s')\n",
    "    s2 = re.sub(pattern_2,'', s1)\n",
    "\n",
    "    # 删除‘@用户名:’\n",
    "    pattern_3 = re.compile('@.*?:')\n",
    "    s3 = re.sub(pattern_3,'', s2)\n",
    "\n",
    "    # 删除‘@用户名,’\n",
    "    pattern_3 = re.compile('@.*?,')\n",
    "    s4 = re.sub(pattern_3,'', s3)\n",
    "\n",
    "    # 删除‘@用户名。’\n",
    "    pattern_3 = re.compile('@.*?。')\n",
    "    s5 = re.sub(pattern_3,'', s4)\n",
    "\n",
    "\n",
    "    d['review'] = s5\n",
    "    return d"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T02:25:56.579991600Z",
     "start_time": "2023-07-18T02:25:56.082258700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\sjj\\.cache\\huggingface\\datasets\\csv\\default-a6fb0b0e0bdd94cc\\0.0.0\\eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d\\cache-7ead21f1d71bab64.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": "(DatasetDict({\n     train: Dataset({\n         features: ['label', 'review'],\n         num_rows: 119988\n     })\n }),\n ['\\ufeff更博了，爆照了，帅的呀，就是越来越爱你！生快傻缺[爱你][爱你][爱你]',\n  '土耳其的事要认真对待[哈哈]，否则直接开除。很是细心，酒店都全部OK啦。',\n  '姑娘都羡慕你呢…还有招财猫高兴……[哈哈]小学徒一枚，等着明天见您呢大佬范儿[书呆子]',\n  '美~~~~~[爱你]',\n  '梦想有多大，舞台就有多大![鼓掌]',\n  '[花心][鼓掌] [春暖花开]',\n  '某问答社区上收到一大学生发给我的私信：“偶喜欢阿姨！偶是阿姨控！”我回他：“阿姨稀饭小盆友！偶是小盆友控！” [哈哈]',\n  '吃货们无不啧啧称奇，好不喜欢！PS:写错一个字！[哈哈]@陈小kitty猫@游子的歌@solo在厦门',\n  '#Sweet Morning#From now on,love yourself,enjoy living then smile.从现在开始，爱自己，享受生活并且微笑。[呵呵] [嘻嘻] [哈哈] [挤眼] [太开心] 早安、甜心们',\n  '【霍思燕剖腹产下“小江江” 老公落泪】今晨9时霍思燕产下一名男婴，宝宝重8斤3两，母子平安。杜江的脸上洋溢着做爸爸的欣喜：宝宝小名叫“小江江”，眼睛像他，鼻子和嘴巴则像霍思燕，看到宝贝就忍不住落泪！恭喜，祝福“小江江”在爱里健康地成长[爱你]...http://t.cn/z8EwSPU',\n  '[鼓掌] 一流的经纪公司是超模的摇篮！[鼓掌] 东方宾利强大的名模军团！',\n  '真好//[害羞]',\n  '第一次见到有花瓣的面膜，一片抵普通面膜好几片 [哈哈]！补水神器啊，一帖见效！ 睡前一片，15分钟超神奇膜法，第二天起来你会发现你脸又白又嫩还有光泽，持续几天皮肤好像剥了壳的鸡蛋一样白白嫩嫩的[太开心]！ 明星推荐，美妆老师私藏的神奇\"膜\"法！8片礼盒装抢购地址>>>（去评论中找链接哦）',\n  '好感动[亲亲]大家都陆陆续续收到超极本尼泊尔的奖品了，没想到你还带着去看瓷房子~祝蜜月快乐哦',\n  '[雪]大象感觉好冷喔。。。  大象放冰箱分三步，绑定手机也分三步。大象放冰箱很冷，微博绑了手机更安全哦～[可爱]',\n  '[哈哈] 探子！[偷笑]  [偷笑] 店里来了探子竟不知 赖老板的货品华丽丽的出镜鸟[鼓掌]',\n  '更要感激部门同事和尚未开博的小徐同学及的辛苦付出，让我们携手前行、踏浪远航，创造属于我们的无限炫彩[爱你]',\n  '陪看，有三陪嫌疑，[哈哈]',\n  '还有 ，各位都是好声音啊，演唱会就星外星筹办了[哈哈]  回复同意的举手 有爱心，期待百首童谣！ 女儿，你有音乐唱了',\n  '第一次见，真心分辨不出是谁是谁，更别提真假了[嘻嘻]'])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wb_data_map = wb_data.map(function=f)\n",
    "\n",
    "wb_data_map,wb_data_map['train']['review'][:20]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T02:25:59.592719100Z",
     "start_time": "2023-07-18T02:25:57.750244600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "原数据集前半是积极label,后半是消极label,需要打乱"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at C:\\Users\\sjj\\.cache\\huggingface\\datasets\\csv\\default-a6fb0b0e0bdd94cc\\0.0.0\\eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d\\cache-1d339c3751a312bc.arrow\n"
     ]
    }
   ],
   "source": [
    "mapped_data = wb_data_map.shuffle(seed=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T02:26:12.032994500Z",
     "start_time": "2023-07-18T02:26:11.756321800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "在 train Dataset 内部再分割出 train set / test set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['label', 'review'],\n        num_rows: 89991\n    })\n    test: Dataset({\n        features: ['label', 'review'],\n        num_rows: 29997\n    })\n})"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = mapped_data['train'].train_test_split(test_size=0.25)\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T02:26:14.895886500Z",
     "start_time": "2023-07-18T02:26:14.626188900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.加载数据集"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-07-18T02:26:19.491854700Z",
     "start_time": "2023-07-18T02:26:18.684828200Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/sjj/.cache/huggingface/datasets/csv/default-a6fb0b0e0bdd94cc/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "485148f46e9c44e5b8997b56d40d6e97"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/sjj/.cache/huggingface/datasets/csv/default-a6fb0b0e0bdd94cc/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a4b8d1fdcb6c41e1af47dae709c542e3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "(89991,\n (1,\n  '卖萌才是终极归宿~~~ 桌面都是各种你，这个封面用了很久 回复[哈哈]恭喜哟~~请吃猫粮哟~~~ 请热烈、紧张、肉紧滴围观！感谢国家，感谢CCAV，感谢围脖儿，感谢俺滴所有粉丝，感谢所有同事，俺，上封面鸟！'),\n 29997,\n (1, '[嘻嘻][嘻嘻]迷倒了'))"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义数据集\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,split):\n",
    "        # 导入本地 csv 文件,得到一个只含 train Dataset 的数据集\n",
    "        wb_data = load_dataset(\"csv\",data_files='../data/weibo_senti_100k.csv')\n",
    "        #\n",
    "        total_data =  wb_data['train'].train_test_split(test_size=0.25)\n",
    "        # 选取训练集或测试集，并打乱顺序\n",
    "        self.dataset = data[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        review = self.dataset[i]['review']\n",
    "        label = self.dataset[i]['label']\n",
    "\n",
    "        return label,review\n",
    "\n",
    "\n",
    "train_dataset = Dataset('train')\n",
    "test_dataset = Dataset('test')\n",
    "\n",
    "\n",
    "len(train_dataset), train_dataset[0],len(test_dataset), test_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.加载分词器\n",
    "通常，一种模型对应一种特殊的 Tokenizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "BertTokenizer(name_or_path='bert-base-chinese', vocab_size=21128, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "#加载字典和分词工具\n",
    "token  = BertTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path='bert-base-chinese',\n",
    "    cache_dir=None,\n",
    "    force_download=False,\n",
    ")\n",
    "\n",
    "token"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T02:27:20.824069900Z",
     "start_time": "2023-07-18T02:27:19.569359Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.更新词典\n",
    "词典要添加所有语句的 jieba 分词结果数组\n",
    "\n",
    "#### （1）打印原有的 vocab，只有单字"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "(dict, 21128, True)"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic = token.get_vocab()\n",
    "type(dic), len(dic), '撒' in dic"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T02:37:40.733079500Z",
     "start_time": "2023-07-18T02:37:40.561204100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### （2）函数，传入一个数据集及指定的列，返回对应去重的词典"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def get_vocab(dataset):\n",
    "    # 检查数据类型\n",
    "    print('dataset.shape:\\n',dataset.shape)\n",
    "    reviews_list = dataset['train']['review']\n",
    "    print(type(reviews_list))\n",
    "\n",
    "    # 分词\n",
    "    total_vocab=[]\n",
    "    for i in range(len(reviews_list)):\n",
    "        # total_vocab 保存每条句子的分词结果\n",
    "        total_vocab += list(jieba.cut(reviews_list[i],cut_all=False))\n",
    "\n",
    "    #total_vocab 去重\n",
    "    return set(total_vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T02:37:45.510502Z",
     "start_time": "2023-07-18T02:37:45.416750700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset.shape:\n",
      " {'train': (119988, 2)}\n",
      "<class 'list'>\n",
      "get_vocab： 62.011430978775024 秒\n",
      "set 相减操作 over\n",
      "去掉长度=1的单字操作 over\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()  # 记录开始时间\n",
    "vocab = get_vocab(mapped_data) # 传入划分数据集/测试集前的总集\n",
    "end_time = time.time()  # 记录结束时间\n",
    "\n",
    "run_time = end_time - start_time  # 计算运行时间\n",
    "print('get_vocab：', run_time, '秒')\n",
    "\n",
    "# 减去 预处理 Bert-Chinese 的tokenizer 的词表中已有的\n",
    "vocab = vocab - set(token.vocab.keys())\n",
    "print(\"set 相减操作 over\")\n",
    "\n",
    "# 去掉长度为 1 的所有字符（要么预处理Bert-Chinese 已经有了，要么就是一些emoji ）\n",
    "vocab = list(vocab)\n",
    "vocab =[token for token in vocab if len(token)>1]\n",
    "print(\"去掉长度=1的单字操作 over\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T03:19:55.989806700Z",
     "start_time": "2023-07-18T03:18:53.850989900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "将构造的新词典导入分词器\n",
    "这篇文章可读：（https://www.depends-on-the-definition.com/how-to-add-new-tokens-to-huggingface-transformers/）"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "(dict, 163958, True)"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 添加新词\n",
    "token.add_tokens(new_tokens=vocab)\n",
    "# 添加新符号\n",
    "token.add_special_tokens({'eos_token':'[EOS]'})\n",
    "\n",
    "d = token.get_vocab()\n",
    "\n",
    "token.save_vocabulary('../data/weibo100k')\n",
    "\n",
    "type(d), len(d), '出发' in d"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T03:22:37.250561400Z",
     "start_time": "2023-07-18T03:22:35.511594500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.定义 DataLoader 与 batch 处理函数\n",
    "\n",
    "> DataLoader 实例对象并不需要移动到 GPU 上，实际上需要移动的是 loader 产生的数据张量\n",
    "> 1/3 定义了 collate_fn ,`.to(device)`应当写在 collate_fn 内"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5624\n"
     ]
    },
    {
     "data": {
      "text/plain": "(torch.Size([16, 200]),\n torch.Size([16, 200]),\n torch.Size([16, 200]),\n tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], device='cuda:0'))"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def collate_fn(data):\n",
    "    labels = [i[0] for i in data]\n",
    "    reviews = [i[1] for i in data]\n",
    "\n",
    "    #编码\n",
    "    data = token.batch_encode_plus(batch_text_or_text_pairs=reviews,\n",
    "                                   truncation=True,\n",
    "                                   padding='max_length',\n",
    "                                   max_length=200,\n",
    "                                   return_tensors='pt',\n",
    "                                   return_length=True)\n",
    "\n",
    "    #input_ids:编码之后的数字\n",
    "    #attention_mask:是补零的位置是0,其他位置是1\n",
    "    input_ids = data['input_ids']\n",
    "    attention_mask = data['attention_mask']\n",
    "    token_type_ids = data['token_type_ids']\n",
    "    labels = torch.LongTensor(labels)\n",
    "\n",
    "    #print(data['length'], data['length'].max())\n",
    "\n",
    "    return input_ids.to(device), attention_mask.to(device), token_type_ids.to(device), labels.to(device)\n",
    "\n",
    "\n",
    "#数据加载器\n",
    "loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                     batch_size=16,\n",
    "                                     collate_fn=collate_fn,\n",
    "                                     shuffle=True,\n",
    "                                     drop_last=True)\n",
    "\n",
    "for i, (input_ids, attention_mask, token_type_ids,\n",
    "        labels) in enumerate(loader):\n",
    "    break\n",
    "\n",
    "print(len(loader))\n",
    "input_ids.shape, attention_mask.shape, token_type_ids.shape, labels"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T03:22:40.578762Z",
     "start_time": "2023-07-18T03:22:39.806538800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 加载 BERT 模型\n",
    "\n",
    "2/3 嵌入的子模型创建了实例，也要搬到GPU"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "#加载预训练模型\n",
    "pretrained = BertModel.from_pretrained('bert-base-chinese')\n",
    "\n",
    "# add new, random embeddings for the new tokens（因为词表更新了，模型的embedding 也要更新）\n",
    "# 如果不加这一行，会出现 CUDA 报错\n",
    "pretrained.resize_token_embeddings(len(token))\n",
    "\n",
    "# 并转移到 GPU ,否则报错\n",
    "pretrained.to(device)\n",
    "#不训练,不需要计算梯度（requires_grad_(False) 用来冻结参数）\n",
    "for param in pretrained.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "\n",
    "\n",
    "# #模型试算\n",
    "# output = pretrained(input_ids=input_ids,\n",
    "#            attention_mask=attention_mask,\n",
    "#            token_type_ids=token_type_ids)\n",
    "#\n",
    "# output.last_hidden_state.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T03:23:02.534846700Z",
     "start_time": "2023-07-18T03:22:52.898745500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T03:23:04.066216600Z",
     "start_time": "2023-07-18T03:23:03.983838400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 定义下游任务模型\n",
    "3/3 总体的 model 搬到GPU"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0204,  0.0273, -0.0168,  ..., -0.0343, -0.0299,  0.0052],\n",
      "        [ 0.0112, -0.0046,  0.0196,  ...,  0.0290,  0.0152,  0.0069]],\n",
      "       device='cuda:0', requires_grad=True) cuda:0\n",
      "Parameter containing:\n",
      "tensor([ 0.0271, -0.0096], device='cuda:0', requires_grad=True) cuda:0\n"
     ]
    }
   ],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 仅新增一个全连接层\n",
    "        self.fc = torch.nn.Linear(768, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        with torch.no_grad():\n",
    "            bert = pretrained(input_ids=input_ids,\n",
    "                       attention_mask=attention_mask,\n",
    "                       token_type_ids=token_type_ids)\n",
    "        # 仅对 <cls> 一个词元做全连接层输出\n",
    "        out = self.fc(bert.last_hidden_state[:, 0])\n",
    "\n",
    "        out = out.softmax(dim=1)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "# 模型搬运到 GPU 上\n",
    "model = Model()\n",
    "model.to(device)\n",
    "for p in model.parameters():\n",
    "    print(p,p.device)\n",
    "\n",
    "# # 数据搬运到 GPU 上\n",
    "# model(input_ids=input_ids.to(device),\n",
    "#       attention_mask=attention_mask.to(device),\n",
    "#       token_type_ids=token_type_ids.to(device)).shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T03:23:08.208120600Z",
     "start_time": "2023-07-18T03:23:08.096077700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 训练"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.6870027780532837 0.5\n",
      "5 0.6656901240348816 0.625\n",
      "10 0.7013100981712341 0.4375\n",
      "15 0.6791286468505859 0.75\n",
      "20 0.6740584969520569 0.625\n",
      "25 0.6870272159576416 0.625\n",
      "30 0.6768355369567871 0.625\n",
      "35 0.6688064336776733 0.5625\n",
      "40 0.6764165759086609 0.5625\n",
      "45 0.63028484582901 0.6875\n",
      "50 0.6485089063644409 0.625\n",
      "55 0.65523761510849 0.6875\n",
      "60 0.6024040579795837 0.875\n",
      "65 0.6281912922859192 0.6875\n",
      "70 0.6185539960861206 0.75\n",
      "75 0.6220785975456238 0.6875\n",
      "80 0.5600926876068115 0.8125\n",
      "85 0.6008599996566772 0.6875\n",
      "90 0.5925366878509521 0.8125\n",
      "95 0.6616859436035156 0.625\n",
      "100 0.6690319180488586 0.5625\n",
      "105 0.5845451354980469 0.875\n",
      "110 0.6158508062362671 0.8125\n",
      "115 0.5583750605583191 0.875\n",
      "120 0.6233053803443909 0.75\n",
      "125 0.5831074714660645 0.875\n",
      "130 0.6018896698951721 0.75\n",
      "135 0.607182502746582 0.6875\n",
      "140 0.5506905913352966 0.875\n",
      "145 0.5384426116943359 0.8125\n",
      "150 0.6007279753684998 0.75\n",
      "155 0.5899836421012878 0.6875\n",
      "160 0.5548563003540039 0.75\n",
      "165 0.638267457485199 0.6875\n",
      "170 0.6301925778388977 0.625\n",
      "175 0.5167123079299927 0.8125\n",
      "180 0.5407338738441467 0.875\n",
      "185 0.5725846290588379 0.875\n",
      "190 0.6268069744110107 0.6875\n",
      "195 0.5377039313316345 0.8125\n",
      "200 0.4976820945739746 0.875\n",
      "205 0.6126130819320679 0.6875\n",
      "210 0.5623968243598938 0.75\n",
      "215 0.5789127349853516 0.8125\n",
      "220 0.5390236973762512 0.8125\n",
      "225 0.5194908380508423 0.9375\n",
      "230 0.5321633219718933 0.875\n",
      "235 0.6553520560264587 0.6875\n",
      "240 0.5149635076522827 0.875\n",
      "245 0.4590805768966675 0.875\n",
      "250 0.573144257068634 0.8125\n",
      "255 0.515272855758667 0.875\n",
      "260 0.5550359487533569 0.8125\n",
      "265 0.5303755402565002 0.8125\n",
      "270 0.60346919298172 0.75\n",
      "275 0.5940033197402954 0.6875\n",
      "280 0.5401530265808105 0.875\n",
      "285 0.4375728964805603 1.0\n",
      "290 0.5065314173698425 0.8125\n",
      "295 0.5085489153862 0.875\n",
      "300 0.5231366157531738 0.875\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "# from torch.optim import AdamW\n",
    "#训练\n",
    "optimizer = AdamW(model.parameters(), lr=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss() # 已经自带 softmax\n",
    "\n",
    "model.train()\n",
    "\n",
    "# 检测是否正确移动到 GPU\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(name, param.shape, param.device)\n",
    "\n",
    "\n",
    "for i, (input_ids, attention_mask, token_type_ids,\n",
    "        labels) in enumerate(loader):\n",
    "    input_ids_gpu = input_ids.to(device)\n",
    "    attention_mask_gpu = attention_mask.to(device)\n",
    "    token_type_ids_gpu = token_type_ids.to(device)\n",
    "    labels_gpu = labels.to(device)\n",
    "\n",
    "\n",
    "    out = model(input_ids=input_ids_gpu,\n",
    "                attention_mask=attention_mask_gpu,\n",
    "                token_type_ids=token_type_ids_gpu)\n",
    "\n",
    "    # print(out.device)\n",
    "\n",
    "    loss = criterion(out, labels_gpu)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if i % 5 == 0:\n",
    "        out = out.argmax(dim=1)\n",
    "        accuracy = (out == labels_gpu).sum().item() / len(labels_gpu)   # 用 item() 取值的精度更高\n",
    "\n",
    "        print(i, loss.item(), accuracy)\n",
    "\n",
    "    if i == 300:\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T03:30:41.321481300Z",
     "start_time": "2023-07-18T03:23:11.892688700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 测试"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/sjj/.cache/huggingface/datasets/csv/default-a6fb0b0e0bdd94cc/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cd054cb95ef04488b3e8309da7d0047a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0.88125\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    loader_test = torch.utils.data.DataLoader(dataset=Dataset('test'),\n",
    "                                              batch_size=32,\n",
    "                                              collate_fn=collate_fn,\n",
    "                                              shuffle=True,\n",
    "                                              drop_last=True)\n",
    "\n",
    "    for i, (input_ids, attention_mask, token_type_ids,labels) \\\n",
    "            in enumerate(loader_test):\n",
    "\n",
    "        if i == 5:\n",
    "            break\n",
    "\n",
    "        print(i)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = model(input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        token_type_ids=token_type_ids)\n",
    "\n",
    "        out = out.argmax(dim=1)\n",
    "        correct += (out == labels).sum().item()\n",
    "        total += len(labels)\n",
    "\n",
    "    print(correct / total)\n",
    "\n",
    "\n",
    "test()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T05:55:32.096099300Z",
     "start_time": "2023-07-18T05:55:10.440534700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
