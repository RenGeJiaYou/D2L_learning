{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "自然语言处理应用可以划分成几个类别：\n",
    "- 序列级\n",
    "    - 单文本分类：情绪分析*\n",
    "    - 文本对分类：自然语言推断\n",
    "- 词元级\n",
    "    - 文本标注\n",
    "    - 问答任务\n",
    "\n",
    "在单文本分类应用中，特殊分类标记“<cls>”的BERT表示对整个输入文本序列的信息进行编码。作为输入单个文本的表示，它将被送入到由全连接（稠密）层组成的小多层感知机中，以输出所有离散标签值的分布。\n",
    "\n",
    "\n",
    "《D2L》使用前一节定义的数据集做自然语言推断，微调 BERT"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.加载预训练 BERT"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-07-13T07:08:18.181054300Z",
     "start_time": "2023-07-13T07:08:09.830594900Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import multiprocessing\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "两个版本的预训练的BERT：\n",
    "“bert.base”与原始的BERT基础模型一样大，需要大量的计算资源才能进行微调，\n",
    "而“bert.small”是一个小版本，以便于演示。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# d2l.DATA_HUB['bert.base'] = (d2l.DATA_URL + 'bert.base.torch.zip',\n",
    "#                              '225d66f04cae318b841a13d32af3acc165f253ac')\n",
    "d2l.DATA_HUB['bert.small'] = (d2l.DATA_URL + 'bert.small.torch.zip',\n",
    "                              'c72329e68a732bef0452e4b96a1c341c8910f81f')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-13T07:13:18.997885400Z",
     "start_time": "2023-07-13T07:13:18.957882Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "预训练好的BERT模型包含\n",
    "- 定义词表的 `vocab.json`\n",
    "- 预训练参数的 `pretrained.params`\n",
    "接下来加载这些参数"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def load_pretrained_model(pretrained_model, num_hiddens, ffn_num_hiddens,\n",
    "                          num_heads, num_layers, dropout, max_len, devices):\n",
    "    data_dir = d2l.download_extract(pretrained_model)\n",
    "    # 定义空词表以加载预定义词表\n",
    "    vocab = d2l.Vocab()\n",
    "    vocab.idx_to_token = json.load(open(os.path.join(data_dir,\n",
    "        'vocab.json')))\n",
    "    vocab.token_to_idx = {token: idx for idx, token in enumerate(\n",
    "        vocab.idx_to_token)}\n",
    "\n",
    "    bert = d2l.BERTModel(len(vocab), num_hiddens, norm_shape=[256],\n",
    "                         ffn_num_input=256, ffn_num_hiddens=ffn_num_hiddens,\n",
    "                         num_heads=4, num_layers=2, dropout=0.2,\n",
    "                         max_len=max_len, key_size=256, query_size=256,\n",
    "                         value_size=256, hid_in_features=256,\n",
    "                         mlm_in_features=256, nsp_in_features=256)\n",
    "    # 加载预训练BERT参数\n",
    "    bert.load_state_dict(torch.load(os.path.join(data_dir,\n",
    "                                                 'pretrained.params')))\n",
    "    return bert, vocab"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-13T07:30:57.787670800Z",
     "start_time": "2023-07-13T07:30:57.735649500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ..\\data\\bert.small.torch.zip from http://d2l-data.s3-accelerate.amazonaws.com/bert.small.torch.zip...\n"
     ]
    }
   ],
   "source": [
    "devices = d2l.try_all_gpus()\n",
    "bert, vocab = load_pretrained_model(\n",
    "    'bert.small', num_hiddens=256, ffn_num_hiddens=512, num_heads=4,\n",
    "    num_layers=2, dropout=0.1, max_len=512, devices=devices)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-13T07:31:27.187541800Z",
     "start_time": "2023-07-13T07:30:58.342329800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. 微调 BERT 数据集"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "对于 SNLI 数据集的下游任务自然语言推断，我们定义了一个定制的数据集类 SNLIBERTDataset。\n",
    "在每个样本中，前提和假设形成一对文本序列，并被打包成一个BERT input vector\n",
    "segment embedding 用于区分 BERT 输入序列中的前提和假设。\n",
    "利用预定义的BERT输入序列的最大长度（max_len），持续移除 输入文本对 中较长文本的最后一个标记，直到满足max_len。\n",
    "为了加速生成用于微调 BERT 的 SNLI 数据集，开 4 个工作进程并行生成 训练/测试样本。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class SNLIBERTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, max_len, vocab=None):\n",
    "        # all_premise_hypothesis_tokens[i][0] 表示第 i 个句子对的“前提句”（该句是一个 list(int)）,以此类推\n",
    "        all_premise_hypothesis_tokens = [\n",
    "            [p_tokens, h_tokens] for p_tokens, h_tokens in zip(\n",
    "            *[d2l.tokenize([s.lower() for s in sentences])\n",
    "              for sentences in dataset[:2]])]\n",
    "\n",
    "        self.labels = torch.tensor(dataset[2])\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "        (self.all_token_ids, self.all_segments,\n",
    "         self.valid_lens) = self._preprocess(all_premise_hypothesis_tokens)\n",
    "        print('read ' + str(len(self.all_token_ids)) + ' examples')\n",
    "\n",
    "    #Python 是单线程的，数据预处理非常慢，要么开多线程，要么用 C++\n",
    "    def _preprocess(self, all_premise_hypothesis_tokens):\n",
    "        pool = multiprocessing.Pool(4)  # 使用4个进程\n",
    "        out = pool.map(self._mp_worker, all_premise_hypothesis_tokens) # map 将函数_mp_worker() 挂载到每个元素上\n",
    "        # 具体来说，all_premise_hypothesis_tokens 中的每个元素都传参到 _mp_worker() 一次，这项工作由4个进程分别同时完成\n",
    "\n",
    "        all_token_ids = [\n",
    "            token_ids for token_ids, segments, valid_len in out]\n",
    "        all_segments = [segments for token_ids, segments, valid_len in out]\n",
    "        valid_lens = [valid_len for token_ids, segments, valid_len in out]\n",
    "\n",
    "        return (torch.tensor(all_token_ids, dtype=torch.long),\n",
    "                torch.tensor(all_segments, dtype=torch.long),\n",
    "                torch.tensor(valid_lens))\n",
    "\n",
    "    def _mp_worker(self, premise_hypothesis_tokens):\n",
    "        p_tokens, h_tokens = premise_hypothesis_tokens\n",
    "        self._truncate_pair_of_tokens(p_tokens, h_tokens)\n",
    "        tokens, segments = d2l.get_tokens_and_segments(p_tokens, h_tokens)\n",
    "\n",
    "        # 完成填充数据的任务\n",
    "        token_ids = self.vocab[tokens] + [self.vocab['<pad>']] * (self.max_len - len(tokens))\n",
    "        segments = segments + [0] * (self.max_len - len(segments))\n",
    "        valid_len = len(tokens)\n",
    "        return token_ids, segments, valid_len\n",
    "\n",
    "    def _truncate_pair_of_tokens(self, p_tokens, h_tokens):\n",
    "        # 为BERT输入中的'<CLS>'、'<SEP>'和'<SEP>'词元保留位置\n",
    "        while len(p_tokens) + len(h_tokens) > self.max_len - 3:\n",
    "            if len(p_tokens) > len(h_tokens):\n",
    "                p_tokens.pop()\n",
    "            else:\n",
    "                h_tokens.pop()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.all_token_ids[idx], self.all_segments[idx],\n",
    "                self.valid_lens[idx]), self.labels[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_token_ids)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-13T08:22:24.046973800Z",
     "start_time": "2023-07-13T08:22:23.848948900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "下载完SNLI数据集后，我们通过实例化SNLIBERTDataset类来生成训练和测试样本。\n",
    "这些样本将在自然语言推断的训练和测试期间进行小批量读取。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 如果出现显存不足错误，请减少“batch_size”。在原始的BERT模型中，max_len=512\n",
    "batch_size, max_len, num_workers = 16, 128, d2l.get_dataloader_workers()\n",
    "# data_dir = d2l.download_extract('SNLI')\n",
    "data_dir = \"../data/snli_1.0\"\n",
    "train_set = SNLIBERTDataset(d2l.read_snli(data_dir, True), max_len, vocab)\n",
    "test_set = SNLIBERTDataset(d2l.read_snli(data_dir, False), max_len, vocab)\n",
    "\n",
    "train_iter = torch.utils.data.DataLoader(train_set, batch_size, shuffle=True,num_workers=num_workers)\n",
    "test_iter = torch.utils.data.DataLoader(test_set, batch_size,num_workers=num_workers)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. 微调 BERT\n",
    "用于自然语言推断的微调BERT只需要一个由两个全连接层组成的MLP。\n",
    "这个多层感知机将特殊的“<cls>”词元的 BERT 表示进行了转换，该词元同时编码前提和假设的信息为自然语言推断的三个输出：蕴涵、矛盾和中性。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, bert):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.encoder = bert.encoder\n",
    "        self.hidden = bert.hidden\n",
    "        self.output = nn.Linear(256, 3)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        tokens_X, segments_X, valid_lens_x = inputs\n",
    "        encoded_X = self.encoder(tokens_X, segments_X, valid_lens_x)\n",
    "        # 保留(batch_size,句首的<cls>,向量长度)\n",
    "        return self.output(self.hidden(encoded_X[:, 0, :]))"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "预训练的BERT模型 bert 被送到用于下游应用的BERTClassifier实例net中。\n",
    "在BERT微调的常见实现中，只有额外的多层感知机（net.output）的输出层的参数将从零开始学习。\n",
    "预训练BERT编码器（net.encoder）和额外的多层感知机的隐藏层（net.hidden）的所有参数都将进行微调。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "net = BERTClassifier(bert)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "预训练的 BERT 中不是所有的参数都会被微调到。\n",
    "\n",
    "MLM 类和 NSP 类在其使用的多层感知机中都有一些参数。这些参数是预训练BERT模型 bert 中参数的一部分，因此也是net中的参数的一部分。\n",
    "然而，这些参数仅用于计算预训练过程中的遮蔽语言模型损失和下一句预测损失。\n",
    "这两个损失函数与微调下游应用无关，因此当BERT微调时，MaskLM和NextSentencePred中采用的多层感知机的参数不会更新（陈旧的，staled）。\n",
    "\n",
    "为了允许具有陈旧梯度的参数，标志 `ignore_stale_grad=True` 在step函数 `d2l.train_batch_ch13` 中被设置。\n",
    "我们通过该函数使用SNLI的训练集（train_iter）和测试集（test_iter）对net模型进行训练和评估。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lr, num_epochs = 1e-4, 5\n",
    "trainer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "loss = nn.CrossEntropyLoss(reduction='none')\n",
    "d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs,devices)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## QA\n",
    "1. 请问Bert微调的时候，是固定预训练模型的参数吗？\n",
    "> 一般不，所有的参数跟着重新训练。但可以试一下，可以加速训练。\n",
    "\n",
    "2. PC 的性能，想跑 BERT 怎么办？\n",
    "> 用模型蒸馏技术把参数降低到原来的1/10"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
