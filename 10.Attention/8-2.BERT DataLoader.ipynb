{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "数据来源于 WikiText-2，一个较小的数据集，特点是：\n",
    "1. 保留了原来的标点符号 ， 适合于下一句预测 。\n",
    "2. 保留了原来的大小写和数字 。\n",
    "3. 每行代表一个段落 ，任意标点符号及其前后的词元之间都有空格 。\n",
    "4. 每个段落最少有两句话 。\n",
    "\n",
    "在定制的数据集上对BERT进行预训练变得越来越流行，（启发：能否换一个数据集发论文）"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-07-09T12:20:14.010281300Z",
     "start_time": "2023-07-09T12:19:45.716774700Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "简单起见，仅用句号`.`作为分隔符来拆分句子。还有更复杂的拆分句子的技术。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "d2l.DATA_HUB['wikitext-2'] = (\n",
    "    'https://s3.amazonaws.com/research.metamind.io/wikitext/'\n",
    "    'wikitext-2-v1.zip', '3c914d17d80b1459be871a5039ac23e752a53cbe')\n",
    "\n",
    "\n",
    "#@save\n",
    "def _read_wiki(data_dir):\n",
    "    # 根据目录名和文件名构建路径\n",
    "    file_name = os.path.join(data_dir, 'wiki.train.tokens')\n",
    "\n",
    "    # 以只读模式打开\n",
    "    # 并使用 `as` 关键字将打开的文件对象赋值（as 实现向右赋值）给变量 `f`\n",
    "    # `with` 语句用于确保文件在使用完毕后被正确关闭。\n",
    "    with open(file_name, 'r') as f:\n",
    "        # 读取文件的所有行，并将其存储在变量 `lines` 中。每一行作为一个字符串元素存储在列表中。\n",
    "        lines = f.readlines()\n",
    "    # `line.strip()：去除每一行两端的空格和换行符。\n",
    "    # .lower()：将每一行中的大写字母转换为小写字母。\n",
    "    # .split(' . ')：使用字符串 `' . '` 作为分隔符将每一行拆分成一个段落中的句子列表。\n",
    "    # for line in lines if len(line.split(' . ')) >= 2`：对于长度大于等于2的行（含有至少两个句子），进行处理。\n",
    "    paragraphs = [line.strip().lower().split(' . ')\n",
    "                  for line in lines if len(line.split(' . ')) >= 2]\n",
    "    random.shuffle(paragraphs)\n",
    "    return paragraphs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-09T12:20:14.034274600Z",
     "start_time": "2023-07-09T12:20:14.018291Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "['b', 'c', 'a', 'e', 'd']"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = [\"a\",\"b\",\"c\",\"d\",\"e\"]\n",
    "random.shuffle(s)\n",
    "s"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-09T12:20:14.128277200Z",
     "start_time": "2023-07-09T12:20:14.046283600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.定义辅助函数\n",
    "首先为BERT的两个预训练任务mlm、nsp 实现辅助函数。\n",
    "这些辅助函数将在稍后将原始文本语料库转换为理想格式的数据集时调用，以预训练BERT。\n",
    "\n",
    "### (1) 生成下一句预测任务的数据\n",
    "BERT 的 NSP 类中有提到：\n",
    "> 在为预训练生成句子对时，有一半的时间它们确实是标签为“真”的连续句子；\n",
    "> 在另一半的时间里，第二个句子是从语料库中随机抽取的，标记为“假”。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def _get_next_sentence(sentence, next_sentence, paragraphs):\n",
    "    if random.random() < 0.5:\n",
    "        is_next = True\n",
    "    else:\n",
    "        # paragraphs是三重列表的嵌套(段落数,每段句子数,每句单词数),选中元素 paragraphs[random_i][random_j]\n",
    "        next_sentence = random.choice(random.choice(paragraphs))\n",
    "        is_next = False\n",
    "    return sentence, next_sentence, is_next"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-09T12:20:14.129278500Z",
     "start_time": "2023-07-09T12:20:14.078277800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "下面的函数通过调用 `_get_next_sentence` 函数从输入 paragraph 生成用于下一句预测的训练样本。\n",
    "这里 `paragraph` 是句子列表，其中每个句子都是词元列表。\n",
    "自变量 `max_len` 指定预训练期间的 BERT 输入序列的最大长度。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def _get_nsp_data_from_paragraph(paragraph, paragraphs, vocab, max_len):\n",
    "    nsp_data_from_paragraph = []\n",
    "    for i in range(len(paragraph) - 1):\n",
    "        tokens_a, tokens_b, is_next = _get_next_sentence(\n",
    "            paragraph[i], paragraph[i + 1], paragraphs)\n",
    "        # 考虑1个'<cls>'词元和2个'<sep>'词元\n",
    "        if len(tokens_a) + len(tokens_b) + 3 > max_len:\n",
    "            continue\n",
    "        tokens, segments = d2l.get_tokens_and_segments(tokens_a, tokens_b)\n",
    "        nsp_data_from_paragraph.append((tokens, segments, is_next))\n",
    "    return nsp_data_from_paragraph"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-09T12:20:14.131275900Z",
     "start_time": "2023-07-09T12:20:14.098275300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### (2) 生成 MLM 任务的数据"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def _replace_mlm_tokens(tokens, candidate_pred_positions, num_mlm_preds, vocab):\n",
    "    # 为遮蔽语言模型的输入创建新的词元副本，其中输入可能包含替换的“<mask>”或随机词元\n",
    "    mlm_input_tokens = [token for token in tokens]\n",
    "    pred_positions_and_labels = []\n",
    "    # 打乱后用于在遮蔽语言模型任务中获取15%的随机词元进行预测\n",
    "    random.shuffle(candidate_pred_positions)\n",
    "    for mlm_pred_position in candidate_pred_positions:\n",
    "        if len(pred_positions_and_labels) >= num_mlm_preds:\n",
    "            break\n",
    "\n",
    "        masked_token = None\n",
    "        # 80%的时间：将词替换为“<mask>”词元\n",
    "        if random.random() < 0.8:\n",
    "            masked_token = '<mask>'\n",
    "        else:\n",
    "            # 10%的时间：保持词不变\n",
    "            if random.random() < 0.5:\n",
    "                masked_token = tokens[mlm_pred_position]\n",
    "            # 10%的时间：用随机词替换该词\n",
    "            else:\n",
    "                masked_token = random.choice(vocab.idx_to_token)\n",
    "        # 完成遮蔽操作\n",
    "        mlm_input_tokens[mlm_pred_position] = masked_token\n",
    "\n",
    "        # 记录下 candidate_pred_positions 单词的下标及原值\n",
    "        pred_positions_and_labels.append((mlm_pred_position, tokens[mlm_pred_position]))\n",
    "    return mlm_input_tokens, pred_positions_and_labels"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-09T12:20:14.148276600Z",
     "start_time": "2023-07-09T12:20:14.129278500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "通过调用前述的_replace_mlm_tokens函数，以下函数将BERT输入序列（tokens）作为输入，并返回\n",
    "1. 输入词元的索引（在可能的词元替换之后）\n",
    "2. 发生预测的词元索引\n",
    "3. 以及这些预测的标签索引。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def _get_mlm_data_from_tokens(tokens, vocab):\n",
    "    # 记录(除了特殊词元外)待预测单词的下标\n",
    "    candidate_pred_positions = []\n",
    "    # tokens是一个字符串列表\n",
    "    for i, token in enumerate(tokens):\n",
    "        # 在遮蔽语言模型任务中不会预测特殊词元\n",
    "        if token in ['<cls>', '<sep>']:\n",
    "            continue\n",
    "        candidate_pred_positions.append(i)\n",
    "\n",
    "    # MLM 任务中预测一个段落中 15% 的随机词元，超出次数不再预测\n",
    "    num_mlm_preds = max(1, round(len(tokens) * 0.15))  # round 是四舍五入\n",
    "    mlm_input_tokens, pred_positions_and_labels = _replace_mlm_tokens(\n",
    "        tokens, candidate_pred_positions, num_mlm_preds, vocab)\n",
    "\n",
    "    # 按照预测词元的下标排序\n",
    "    pred_positions_and_labels = sorted(pred_positions_and_labels, key=lambda x: x[0])\n",
    "    pred_positions = [v[0] for v in pred_positions_and_labels]\n",
    "    mlm_pred_labels = [v[1] for v in pred_positions_and_labels]\n",
    "\n",
    "    return vocab[mlm_input_tokens], pred_positions, vocab[mlm_pred_labels]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-09T12:20:14.296275500Z",
     "start_time": "2023-07-09T12:20:14.161275400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 二、将文本转换为预训练数据集\n",
    "辅助函数 `_pad_bert_inputs()` 来将特殊的 `<mask>` 词元附加到输入。\n",
    "其参数`examples`包含来自两个预训练任务的辅助函数 `_get_nsp_data_from_paragraph` 和 `_get_mlm_data_from_tokens` 的输出。\n",
    "主要工作：将长度不一的句子统统填充长度到`<pad>`  max_len 。其它相关变量也同样处理"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def _pad_bert_inputs(examples, max_len, vocab):\n",
    "    max_num_mlm_preds = round(max_len * 0.15)\n",
    "    all_token_ids, all_segments, valid_lens, = [], [], []\n",
    "    all_pred_positions, all_mlm_weights, all_mlm_labels = [], [], []\n",
    "    nsp_labels = []\n",
    "\n",
    "    for (token_ids, pred_positions, mlm_pred_label_ids, segments, is_next) in examples:\n",
    "        # 句子有长有短，为了保持数据长度一致，在不足 max_len 长的句子后面补 '<pad>'\n",
    "        all_token_ids.append(torch.tensor(token_ids + [vocab['<pad>']] * (\n",
    "                max_len - len(token_ids)), dtype=torch.long))\n",
    "        all_segments.append(torch.tensor(segments + [0] * (\n",
    "                max_len - len(segments)), dtype=torch.long))\n",
    "        # valid_lens不包括'<pad>'的计数，valid_lens 表示每句中自然词元的个数。\n",
    "        valid_lens.append(torch.tensor(len(token_ids), dtype=torch.float32))\n",
    "\n",
    "        # 汇集所有段落的预测位置，亦做填充至 max_num_mlm_preds 等长\n",
    "        all_pred_positions.append(torch.tensor(pred_positions + [0] * (\n",
    "                max_num_mlm_preds - len(pred_positions)), dtype=torch.long))\n",
    "\n",
    "        # 填充词元的预测将通过乘以0权重在损失中过滤掉\n",
    "        all_mlm_weights.append(\n",
    "            torch.tensor([1.0] * len(mlm_pred_label_ids) + [0.0] * (\n",
    "                    max_num_mlm_preds - len(pred_positions)),\n",
    "                         dtype=torch.float32))\n",
    "\n",
    "        all_mlm_labels.append(torch.tensor(mlm_pred_label_ids + [0] * (\n",
    "                max_num_mlm_preds - len(mlm_pred_label_ids)), dtype=torch.long))\n",
    "\n",
    "        nsp_labels.append(torch.tensor(is_next, dtype=torch.long))\n",
    "\n",
    "    return (all_token_ids, all_segments, valid_lens, all_pred_positions,\n",
    "            all_mlm_weights, all_mlm_labels, nsp_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-09T12:20:14.339276400Z",
     "start_time": "2023-07-09T12:20:14.182278500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "将用于`生成两个预训练任务的训练样本` 的辅助函数和用于 `填充输入` 的辅助函数放在一起，\n",
    "定义以下_WikiTextDataset类为用于预训练BERT的WikiText-2数据集。\n",
    "通过实现`__getitem__`函数，我们可以任意访问 WikiText-2 语料库的一对句子生成的预训练样本（遮蔽语言模型和下一句预测）样本。\n",
    "\n",
    "最初的BERT模型使用词表大小为 `30000` 的 WordPiece 嵌入。\n",
    "为简单起见，我们使用 d2l.tokenize 函数进行词元化。出现次数少于5次的不频繁词元将被过滤掉。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class _WikiTextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, paragraphs, max_len):\n",
    "        # 输入paragraphs[i]是代表段落的句子字符串列表；\n",
    "        # 而输出 paragraphs[i] 是代表段落的句子列表，其中每个句子都是词元列表。paragraphs[i][j][k] 表示第 i 个段落中第 j 句的第 k 个词元\n",
    "        # sentences 是一个二维张量，把所有段落中的句子整合到一起。sentences[i][j] 表示第 i 个句子中第 j 个单词\n",
    "        paragraphs = [d2l.tokenize(\n",
    "            paragraph, token='word') for paragraph in paragraphs]\n",
    "        sentences = [sentence for paragraph in paragraphs\n",
    "                     for sentence in paragraph]\n",
    "        self.vocab = d2l.Vocab(sentences, min_freq=5, reserved_tokens=['<pad>', '<mask>', '<cls>', '<sep>'])\n",
    "\n",
    "        # 获取 NSP 任务的数据\n",
    "        examples = []\n",
    "        for paragraph in paragraphs:\n",
    "            examples.extend(_get_nsp_data_from_paragraph(\n",
    "                paragraph, paragraphs, self.vocab, max_len))\n",
    "\n",
    "        # 获取 MLM 任务的数据\n",
    "        examples = [(_get_mlm_data_from_tokens(tokens, self.vocab)\n",
    "                     + (segments, is_next))\n",
    "                    for tokens, segments, is_next in examples]\n",
    "        # 填充输入\n",
    "        (self.all_token_ids, self.all_segments, self.valid_lens,\n",
    "         self.all_pred_positions, self.all_mlm_weights,\n",
    "         self.all_mlm_labels, self.nsp_labels) = \\\n",
    "            _pad_bert_inputs(examples, max_len, self.vocab)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.all_token_ids[idx], self.all_segments[idx],\n",
    "                self.valid_lens[idx], self.all_pred_positions[idx],\n",
    "                self.all_mlm_weights[idx], self.all_mlm_labels[idx],\n",
    "                self.nsp_labels[idx])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_token_ids)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-09T12:20:14.341272800Z",
     "start_time": "2023-07-09T12:20:14.239275500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def load_data_wiki(batch_size, max_len):\n",
    "    \"\"\"加载WikiText-2数据集\"\"\"\n",
    "    num_workers = d2l.get_dataloader_workers()\n",
    "    data_dir = d2l.download_extract('wikitext-2', 'wikitext-2')\n",
    "    paragraphs = _read_wiki(data_dir)\n",
    "    train_set = _WikiTextDataset(paragraphs, max_len)\n",
    "    train_iter = torch.utils.data.DataLoader(train_set, batch_size,\n",
    "                                             shuffle=True, num_workers=num_workers)\n",
    "    return train_iter, train_set.vocab"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-09T12:20:14.343275400Z",
     "start_time": "2023-07-09T12:20:14.258274100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_size, max_len = 512, 64\n",
    "train_iter, vocab = load_data_wiki(batch_size, max_len)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_size, max_len = 512, 64\n",
    "train_iter, vocab = load_data_wiki(batch_size, max_len)\n",
    "\n",
    "for (tokens_X, segments_X, valid_lens_x, pred_positions_X, mlm_weights_X,\n",
    "     mlm_Y, nsp_y) in train_iter:\n",
    "    print(tokens_X.shape, segments_X.shape, valid_lens_x.shape,\n",
    "          pred_positions_X.shape, mlm_weights_X.shape, mlm_Y.shape,\n",
    "          nsp_y.shape)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-07-09T12:20:14.298278200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
