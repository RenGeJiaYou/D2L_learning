{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Bert(Bidirectional Encoder Representation from Transformers) 是个只有**双向编码器**的 Transformer，虽然结构很简单，但效果很好\n",
    "分为两个版本：\n",
    "- Base: #blocks 12, hidden size 768,    #heads 12,  #parameters 110M\n",
    "- Large:#blocks 24, hidden size 1024,   #heads 16,  #parameters 340M\n",
    "\n",
    "创新点：\n",
    "- 输入\n",
    "- Loss 函数\n",
    "\n",
    "<cls> 表示句子的分类，<sep> 表示两句话之间的断句符号\n",
    "\n",
    "### 训练任务1：带掩码的语言模型\n",
    "Bert 之所以是双向的，是因为它不需要预测未来，只根据上下文做完形填空。\n",
    "带掩码的语言模型每次随机（15%概率)将一些词元进行替换\n",
    "因为微调任务中不出现\\<mask>\n",
    "- 80%概率下，将选中的词元变成\\<mask>\n",
    "- 10%概率下换成一个随机词元\n",
    "- 10%概率下保持原有的词元\n",
    "\n",
    "目的是不要看到 mask 就预测\n",
    "\n",
    "### 训练任务2：下一句子预测\n",
    "- 预测一个句子对中两个句子是不是相邻\n",
    ">每次给出一对句子，预测句子在原始数据中是不是相邻的。\n",
    "\n",
    "- 因此，在训练样本中：\n",
    "    - 50% 概率选择**相邻**句子对：<cls>this movie is great<Sep>i like it <sep>\n",
    "    - 50% 概率选择**随机**句子对：<cls>this movie is great<Sep>hello world<sep>\n",
    "\n",
    "### 总结\n",
    "- BERT针对微调设计\n",
    "- 基于Transformer的编码器做了如下修改\n",
    "    - 模型更大，训练数据更多\n",
    "    - 输入句子对，片段嵌入，可学习的位置编码\n",
    "- 训练时使用两个任务：\n",
    "    - 带掩码的语言模型\n",
    "    - 下一个句子预测"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bert 模型"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-07-10T03:16:26.828143200Z",
     "start_time": "2023-07-10T03:16:26.641098300Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. 输入表示\n",
    "NLP 有些任务（如情感分析）以单个文本作为输入，而有些任务（如自然语言推断）以一对文本序列作为输入。bert 对此进行了明确的区分。\n",
    "- 输入为单文本时，输入序列为 \\<cls> + 文本序列(标记化表示)$e_A$ + \\<sep>。\n",
    "- 输入为文本对时，输入序列为 \\<cls> + 第一个文本序列的标记$e_A$ + \\<sep> + 第二个文本序列标记$e_B$ + \\<sep>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "def get_tokens_and_segments(tokens_a, tokens_b=None):\n",
    "    \"\"\"获取输入序列的词元及其片段索引\"\"\"\n",
    "    tokens = ['<cls>'] + tokens_a + ['<sep>']\n",
    "    # 0和1分别标记片段A和B\n",
    "    segments = [0] * (len(tokens_a) + 2)\n",
    "    if tokens_b is not None:\n",
    "        tokens += tokens_b + ['<sep>']\n",
    "        segments += [1] * (len(tokens_b) + 1)\n",
    "    # 返回的是一个单词数组，及等大的段嵌入标记数组(标记当前词元属于 e_A 还是 e_B)\n",
    "    return tokens, segments"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T03:16:28.939117400Z",
     "start_time": "2023-07-10T03:16:28.679926200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "BERT输入序列的嵌入是**词元嵌入**、**片段嵌入**和**位置嵌入**的和。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "BERTEncoder 类类似于 TransformerEncoder 类。\n",
    "但不同在于，BERTEncoder 使用片段嵌入和可学习的位置嵌入。「可学习的位置嵌入」意思是变量应该被nn.Parameter() 包裹"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "class BERTEncoder(nn.Module):\n",
    "    \"\"\"BERT编码器\"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens, norm_shape, ffn_num_input,ffn_num_hiddens,\n",
    "                 num_heads, num_layers, dropout,\n",
    "                 max_len=1000, key_size=768, query_size=768, value_size=768,\n",
    "                 **kwargs):\n",
    "        super(BERTEncoder, self).__init__(**kwargs)\n",
    "        self.token_embedding = nn.Embedding(vocab_size, num_hiddens) # 返回一个(vocab_size, num_hiddens)的可学习矩阵\n",
    "\n",
    "        #  BERT 任务只分单句(用 0 表示)和句子对(两句分别用 0 和 1 表示)，因此 num_embeddings == 2\n",
    "        self.segment_embedding = nn.Embedding(2, num_hiddens)\n",
    "\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.blks.add_module(f\"{i}\", d2l.EncoderBlock(\n",
    "                key_size, query_size, value_size, num_hiddens, norm_shape,\n",
    "                ffn_num_input, ffn_num_hiddens, num_heads, dropout, True))\n",
    "        # 在BERT中，位置嵌入是可学习的，因此我们随机初始化一个足够长的位置嵌入参数\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, max_len,num_hiddens))\n",
    "\n",
    "    def forward(self, tokens, segments, valid_lens):\n",
    "        # 在以下代码段中，X的形状保持不变：（批量大小，最大序列长度，num_hiddens）\n",
    "        X = self.token_embedding(tokens) + self.segment_embedding(segments)\n",
    "\n",
    "        # 位置编码只传入当前句子的实际长度，而非 max_len\n",
    "        X = X + self.pos_embedding.data[:, :X.shape[1], :]\n",
    "        # 数据要在每一层间传递更新\n",
    "        for blk in self.blks:\n",
    "            X = blk(X, valid_lens)\n",
    "        return X"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T03:16:31.285297200Z",
     "start_time": "2023-07-10T03:16:31.066552800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "设词表大小为10000，为了演示 BERTEncoder 的前向推断，让我们创建一个实例并初始化它的参数。\n",
    "\n",
    "## Encoder 的 num_hiddens == 前馈神经网络子层的 ffn_num_input == ADD&Norm 层的 norm_shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "vocab_size, num_hiddens, ffn_num_hiddens, num_heads = 10000, 768, 1024, 4\n",
    "norm_shape, ffn_num_input, num_layers, dropout = [768], 768, 2, 0.2\n",
    "encoder = BERTEncoder(vocab_size, num_hiddens, norm_shape, ffn_num_input,\n",
    "                      ffn_num_hiddens, num_heads, num_layers, dropout)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T03:16:33.180349Z",
     "start_time": "2023-07-10T03:16:32.174475400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 8, 768])"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = torch.randint(0, vocab_size, (2, 8)) # 表示一批数据，包括 2 个长度均为 8 个单词的句子，每个单词的 BERT 输出是一个 768 维(构造编码器时的num_hiddens 参数)的向量\n",
    "segments = torch.tensor([[0, 0, 0, 0, 1, 1, 1, 1], [0, 0, 0, 1, 1, 1, 1, 1]])\n",
    "\n",
    "encoded_X = encoder(tokens, segments, None)\n",
    "encoded_X.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T03:16:33.762749200Z",
     "start_time": "2023-07-10T03:16:33.165346400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "↑ batch_size = 2;最长句的长度max_len = 8;每个单词的 BERT 嵌入向量长度=768"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. 预训练任务\n",
    "BERTEncoder的前向推断给出了输入文本的每个词元和插入的特殊标记`\"<cls>\"`及`\"<seq>\"`的BERT表示。\n",
    "接下来，我们将使用这些表示来计算预训练 BERT 的损失函数。预训练包括以下两个任务：掩蔽语言模型和下一句预测。\n",
    "\n",
    "#### (1) 任务一：遮蔽语言模型 Masked Language Modeling\n",
    "为了双向编码上下文以表示每个词元，BERT (1)随机掩蔽词元(2)并使用来自双向上下文的词元以自监督的方式**预测**掩蔽词元。此任务称为掩蔽语言模型。\n",
    "\n",
    "**具体操作**：\n",
    "随机遮蔽 15% 的词元，作为预测的掩蔽词元，替换规则为：\n",
    "- 80% 时间用 `\"<mask>\"` 替换(例如，“this movie is great”变为“this movie is \\<mask>”)\n",
    "- 10% 时间为随机词元(例如，“this movie is great”变为“this movie is drink”)\n",
    "- 10% 时间内不变(例如，“this movie is great”变为“this movie is great”)\n",
    "\n",
    "设置 10% 的随机词元的好处在于，BERT在其双向上下文编码中不那么偏向于掩蔽词元（尤其是当标签词元保持不变时）。\n",
    "\n",
    "下面的 MaskLM 类来预测 BERT 预训练的掩蔽语言模型任务中的掩蔽标记。\n",
    "MLM 预测使用单隐藏层的多层感知机（self.mlp）。在前向推断中，它需要两个输入：\n",
    "1. BERTEncoder的编码结果\n",
    "2. 用于预测的词元位置。\n",
    "输出是这些位置的预测结果。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "class MaskLM(nn.Module):\n",
    "    \"\"\"BERT的掩蔽语言模型任务\"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens, num_inputs=768, **kwargs):\n",
    "        super(MaskLM, self).__init__(**kwargs)\n",
    "        # 这个单隐藏层的 MLP 接受的是BERT的输出 (batch_size,max_len,BERTEncoder.num_hiddens=768)\n",
    "        # 因此 mlp.num_input = BERTEncoder.num_hiddens = 768\n",
    "        # 形参里的 num_hiddens 属于 MaskLM ，不要与 BERTEncoder.num_hiddens 混淆\n",
    "        self.mlp = nn.Sequential(nn.Linear(num_inputs, num_hiddens),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.LayerNorm(num_hiddens),\n",
    "                                 nn.Linear(num_hiddens, vocab_size)) # 最终是要识别是哪个单词，输出层=vocab_size\n",
    "\n",
    "    # X                 是 BERTEncoder 的输出    (batch_size,max_len,num_hidden=768)\n",
    "    # pred_positions    用来指示被遮蔽的词的位置    (batch_size,max_len)\n",
    "    def forward(self, X, pred_positions):\n",
    "        # 获取 max_len ,即最长句子的单词数/列数\n",
    "        num_pred_positions = pred_positions.shape[1]\n",
    "\n",
    "        # reshape(-1)表示将(m,n,k)多维张量转换为长为 (m*n*k,) 的一维张量。\n",
    "        pred_positions = pred_positions.reshape(-1)\n",
    "\n",
    "        # 举例假设batch_size=2，num_pred_positions=3。\n",
    "        batch_size = X.shape[0]\n",
    "        batch_idx = torch.arange(0, batch_size) # batch_idx = np.array([0,1])\n",
    "        batch_idx = torch.repeat_interleave(batch_idx, num_pred_positions) # batch_idx = np.array([0,0,0,1,1,1])\n",
    "\n",
    "        # 假设 pred_positions = [[1, 5, 2], [6, 1, 5]]\n",
    "        # masked_X = [X[0,1],X[0,5],X[0,2],X[1,6],X[1,1],X[1,5]]  masked_X.shape = (6,768)\n",
    "        masked_X = X[batch_idx, pred_positions]\n",
    "        masked_X = masked_X.reshape((batch_size, num_pred_positions, -1)) # masked_X.shape = (2,3,768)\n",
    "        mlm_Y_hat = self.mlp(masked_X)\n",
    "        # masked_Y.shape = (2,3,vocab_size)\n",
    "        return mlm_Y_hat"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T03:16:36.348772500Z",
     "start_time": "2023-07-10T03:16:36.118319900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "演示1-2 MaskLM 中 forward() 内 masked_X 的实质是选择 batch 中的每句话中的被遮蔽单词"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([6, 768])"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_idx = torch.tensor([0,0,0,1,1,1])\n",
    "mlm_positions = torch.tensor([1, 5, 2,6, 1, 5])\n",
    "masked_X = encoded_X[batch_idx, mlm_positions]\n",
    "masked_X.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T03:16:38.233371200Z",
     "start_time": "2023-07-10T03:16:37.620039400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "演示2-2 MaskLM 的forward()\n",
    "来自 BERTEncoder 的正向推断 encoded_X 表示 2 个 BERT 输入序列。\n",
    "我们将 mlm_positions 定义为在 encoded_X 的任一输入序列中预测的3个指示(即 batch 中每句话都要求遮蔽 3 个元素)。\n",
    "mlm 的前向推断返回encoded_X 的所有掩蔽位置 mlm_positions 处的预测结果 mlm_Y_hat 。\n",
    "每个预测 = vocab_size。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 3, 10000])"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlm = MaskLM(vocab_size, num_hiddens)\n",
    "mlm_positions = torch.tensor([[1, 5, 2], [6, 1, 5]])\n",
    "mlm_Y_hat = mlm(encoded_X, mlm_positions)\n",
    "mlm_Y_hat.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T03:16:40.020640100Z",
     "start_time": "2023-07-10T03:16:39.328673900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "通过掩码下的预测词元mlm_Y的真实标签mlm_Y_hat，我们可以计算在BERT预训练中的遮蔽语言模型任务的交叉熵损失。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([6])"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlm_Y = torch.tensor([[7, 8, 9], [10, 20, 30]])\n",
    "loss = nn.CrossEntropyLoss(reduction='none')\n",
    "mlm_l = loss(mlm_Y_hat.reshape((-1, vocab_size)), mlm_Y.reshape(-1))\n",
    "mlm_l.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T03:16:41.680518Z",
     "start_time": "2023-07-10T03:16:41.193518600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### (2) 任务二：下一句预测\n",
    "尽管掩蔽语言建模能够编码双向上下文来表示单词，但它不能显式地**建模文本对之间的逻辑关系**。\n",
    "为了帮助理解两个文本序列之间的关系，设置下一句预测类 NextSentencePred 。\n",
    "\n",
    "在为预训练生成句子对时，有一半的时间它们确实是标签为“真”的连续句子；在另一半的时间里，第二个句子是从语料库中随机抽取的，标记为“假”。\n",
    "NextSentencePred 类使用单隐藏层的 MLP 来预测 *第二个句子是否是BERT输入序列中第一个句子的下一个句子*,其(输入单元数,隐藏单元数，输出单元数)为：\n",
    "$$hid\\_in\\_features=768,num\\_hiddens=768,2$$\n",
    "\n",
    "由于Transformer编码器中的自注意力，特殊词元`\"<cls>\"`的BERT表示已经对输入的两个句子进行了编码。\n",
    "因此，多层感知机分类器的输出层（self.output）以X作为输入，其中X是多层感知机隐藏层的输出，而MLP隐藏层的输入是编码后的`<cls>`词元。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "class NextSentencePred(nn.Module):\n",
    "    \"\"\"BERT的下一句预测任务\"\"\"\n",
    "    def __init__(self, num_inputs, **kwargs):\n",
    "        super(NextSentencePred, self).__init__(**kwargs)\n",
    "        self.output = nn.Linear(num_inputs, 2) # 只做真/假判断\n",
    "\n",
    "    def forward(self, X):\n",
    "        # X的形状：(batchsize,num_hiddens)\n",
    "        return self.output(X)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T03:16:44.723529400Z",
     "start_time": "2023-07-10T03:16:43.981562Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "# hid_in_features 是 BERTEncoder 的隐藏层单元数,最终作为 BERTEncoder 输出张量的最后一维\n",
    "# hid_in_features 作为 NSP 输入层单元数，num_inputs 作为隐藏层单元数.本例中这两个参数都是 768,NSP 输出层单元数=2\n",
    "# 另外，实参 nsp_in_hidden == 形参 num_inputs == 隐藏层单元数 == 768\n",
    "class NextSentencePred2(nn.Module):\n",
    "    \"\"\"BERT的下一句预测任务\"\"\"\n",
    "    def __init__(self, hid_in_features,num_inputs, **kwargs):\n",
    "        super(NextSentencePred2, self).__init__(**kwargs)\n",
    "        self.hidden = nn.Sequential(nn.Linear(hid_in_features, num_inputs),\n",
    "                                    nn.Tanh())\n",
    "        self.output = nn.Linear(num_inputs, 2) # 只做真/假判断\n",
    "\n",
    "    def forward(self, X):\n",
    "        Y = self.hidden(X)\n",
    "        # X的形状：(batch_size,num_hiddens)\n",
    "        return self.output(Y)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T03:16:45.832032500Z",
     "start_time": "2023-07-10T03:16:45.619031300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "返回每个 BERT 输入序列的二分类预测"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 2])"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_X = torch.flatten(encoded_X, start_dim=1)\n",
    "# NSP的输入形状:(batchsize，num_hiddens)\n",
    "nsp = NextSentencePred(encoded_X.shape[-1])\n",
    "nsp_Y_hat = nsp(encoded_X)\n",
    "nsp_Y_hat.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T03:16:47.925035Z",
     "start_time": "2023-07-10T03:16:47.665029100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2])"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算两个二元分类的交叉熵损失。\n",
    "nsp_y = torch.tensor([0, 1])\n",
    "nsp_l = loss(nsp_Y_hat, nsp_y)\n",
    "nsp_l.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T03:16:48.791606500Z",
     "start_time": "2023-07-10T03:16:48.530607100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. 整合代码\n",
    "在预训练BERT时，最终的损失函数是掩蔽语言模型损失函数和下一句预测损失函数的线性组合。\n",
    "froward() 返回\n",
    "- 编码后的 BERT 表示 `encoded_X`\n",
    "- 掩蔽语言模型预测 `mlm_Y_hat`\n",
    "- 下一句预测 `nsp_Y_hat`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "class BERTModel2(nn.Module):\n",
    "    \"\"\"BERT模型\"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens, norm_shape, ffn_num_input,\n",
    "                 ffn_num_hiddens, num_heads, num_layers, dropout,\n",
    "                 max_len=1000, key_size=768, query_size=768, value_size=768,\n",
    "                 mlm_in_features=768,hid_in_features=768,nsp_in_features=768):\n",
    "        super(BERTModel2, self).__init__()\n",
    "        self.encoder = BERTEncoder(vocab_size, num_hiddens, norm_shape,\n",
    "                    ffn_num_input, ffn_num_hiddens, num_heads, num_layers,\n",
    "                    dropout, max_len=max_len, key_size=key_size,\n",
    "                    query_size=query_size, value_size=value_size)\n",
    "\n",
    "        self.mlm = MaskLM(vocab_size, num_hiddens, mlm_in_features)\n",
    "\n",
    "        # BERTEncoder 的隐藏层单元数(num_hiddens=768) 就是 NSP 层输入单元数(hid_in_features=768)\n",
    "        # NSP 层输入单元数：nsp_in_features=768,发现就是原 self.hidden 的 num_hiddens 参数，而原 self.hidden 的 num_hiddens 参数就是BERTEncoder的 num_hidden，因此直接改成 num_hidden\n",
    "        # self.nsp = NextSentencePred2(hid_in_features,nsp_in_features)\n",
    "        self.nsp = NextSentencePred2(hid_in_features,num_hiddens)\n",
    "\n",
    "\n",
    "    def forward(self, tokens, segments,valid_lens=None,pred_positions=None):\n",
    "        encoded_X = self.encoder(tokens, segments, valid_lens)\n",
    "        if pred_positions is not None:\n",
    "            mlm_Y_hat = self.mlm(encoded_X, pred_positions)\n",
    "        else:\n",
    "            mlm_Y_hat = None\n",
    "        # 用于下一句预测的多层感知机分类器的隐藏层，0是“<cls>”标记的索引,对应一个长为 num_in_feature 的嵌入向量\n",
    "        # 关于重构代码：原 self.hidden 调整数据尺寸 hid_in_features -> num_hiddens\n",
    "        nsp_Y_hat = self.nsp(encoded_X[:, 0, :]) # (batch_size,1,hid_in_features) -> (batch_size,1,num_hiddens) -> (batch_size,1,2)\n",
    "        return encoded_X, mlm_Y_hat, nsp_Y_hat"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T03:16:51.245737200Z",
     "start_time": "2023-07-10T03:16:50.574321800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "class BERTModel(nn.Module):\n",
    "    \"\"\"BERT模型\"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens, norm_shape, ffn_num_input,\n",
    "                 ffn_num_hiddens, num_heads, num_layers, dropout,\n",
    "                 max_len=1000, key_size=768, query_size=768, value_size=768,\n",
    "                 hid_in_features=768, mlm_in_features=768,nsp_in_features=768):\n",
    "        super(BERTModel, self).__init__()\n",
    "        self.encoder = BERTEncoder(vocab_size, num_hiddens, norm_shape,\n",
    "                    ffn_num_input, ffn_num_hiddens, num_heads, num_layers,\n",
    "                    dropout, max_len=max_len, key_size=key_size,\n",
    "                    query_size=query_size, value_size=value_size)\n",
    "\n",
    "        # 此处 hidden 层被单独提取出来，是因为后面有个句子对任务（自然语言推断）需用到该隐藏层做输出 MLP 的一部分\n",
    "        self.hidden = nn.Sequential(nn.Linear(hid_in_features, num_hiddens),\n",
    "                                    nn.Tanh())\n",
    "        self.mlm = MaskLM(vocab_size, num_hiddens, mlm_in_features)\n",
    "        self.nsp = NextSentencePred(nsp_in_features)\n",
    "\n",
    "    def forward(self, tokens, segments, valid_lens=None,pred_positions=None):\n",
    "        encoded_X = self.encoder(tokens, segments, valid_lens)\n",
    "        if pred_positions is not None:\n",
    "            mlm_Y_hat = self.mlm(encoded_X, pred_positions)\n",
    "        else:\n",
    "            mlm_Y_hat = None\n",
    "        # 用于下一句预测的多层感知机分类器的隐藏层，0是“<cls>”标记的索引,对应一个长为 num_in_feature 的嵌入向量\n",
    "        nsp_Y_hat = self.nsp(self.hidden(encoded_X[:, 0, :])) # (batch_size,1,hid_in_features) -> (batch_size,1,num_hiddens) -> (batch_size,1,2)\n",
    "        return encoded_X, mlm_Y_hat, nsp_Y_hat"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T03:16:58.084373400Z",
     "start_time": "2023-07-10T03:16:57.995370400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## QA\n",
    "10%的词会被替换成随机次元的原因：\n",
    "作者在论文中谈到了采取上面的mask策略的好处。大致是说采用上面的策略后，Transformer encoder就不知道会让其预测哪个单词，或者说不知道哪个单词会被随机单词给替换掉，那么它就不得不保持每个输入token的一个上下文的表征分布(a distributional contextual representation)。也就是说如果模型学习到了要预测的单词是什么，那么就会丢失对上下文信息的学习，而如果模型训练过程中无法学习到哪个单词会被预测，那么就必须通过学习上下文的信息来判断出需要预测的单词，这样的模型才具有对句子的特征表示能力。另外，由于随机替换相对句子中所有tokens的发生概率只有1.5%(即15%的10%)，所以并不会影响到模型的语言理解能力。（网上复制的，这是我找到的可以说服我自己的一个解释）"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
