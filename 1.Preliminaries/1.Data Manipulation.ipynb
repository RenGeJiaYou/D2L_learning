{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 一、创建 `Tensor`\n",
    "1.根据函数创建"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "#\n",
    "# x = torch.empty(3, 5)\n",
    "# print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5604, 0.6829, 0.5211],\n",
      "        [0.2641, 0.8667, 0.0666],\n",
      "        [0.6212, 0.8571, 0.6720],\n",
      "        [0.0360, 0.7886, 0.5415],\n",
      "        [0.6624, 0.3290, 0.3057]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T11:40:07.181423Z",
     "end_time": "2023-04-19T11:40:07.421420Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(2, 3, dtype=torch.long)\n",
    "print(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T11:40:07.335425Z",
     "end_time": "2023-04-19T11:40:07.514421Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.根据数据创建(注意数据类型不一致，将自动向上转型)："
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.5000, 6.0000])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([5.5, 6])\n",
    "print(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T11:40:07.371421Z",
     "end_time": "2023-04-19T11:40:07.871436Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3.根据现有的 `Tensor` 创建，默认复用原 `Tensor` 的尺寸、数据类型"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1., 1.]], dtype=torch.float64)\n",
      "tensor([[-1.5758, -1.3541,  1.4405,  0.8051, -1.6108,  0.5364]])\n"
     ]
    }
   ],
   "source": [
    "x = x.new_ones(1, 6, dtype=torch.float64)\n",
    "y = torch.randn_like(x, dtype=torch.float)\n",
    "\n",
    "print(x)\n",
    "print(y)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T11:40:07.479426Z",
     "end_time": "2023-04-19T11:40:07.939447Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "`Tensor` 的尺寸用 `shape` 或者 `size()` 来获取"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6])\n",
      "torch.Size([1, 6])\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)\n",
    "print(y.size())  # 返回值实质是一个 tuple\n",
    "print(y.numel())   # 元素个数"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T11:40:28.482920Z",
     "end_time": "2023-04-19T11:40:28.668904Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 二、操作\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. 算术操作"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.2141, 0.6376, 1.7237],\n",
      "        [1.4952, 0.9937, 1.2871],\n",
      "        [0.6542, 1.4554, 0.6929],\n",
      "        [1.8511, 1.5853, 0.8538],\n",
      "        [0.8609, 0.8204, 1.2031]])\n",
      "tensor([[1.2141, 0.6376, 1.7237],\n",
      "        [1.4952, 0.9937, 1.2871],\n",
      "        [0.6542, 1.4554, 0.6929],\n",
      "        [1.8511, 1.5853, 0.8538],\n",
      "        [0.8609, 0.8204, 1.2031]])\n",
      "tensor([[1.2141, 0.6376, 1.7237],\n",
      "        [1.4952, 0.9937, 1.2871],\n",
      "        [0.6542, 1.4554, 0.6929],\n",
      "        [1.8511, 1.5853, 0.8538],\n",
      "        [0.8609, 0.8204, 1.2031]])\n",
      "tensor([[1.2141, 0.6376, 1.7237],\n",
      "        [1.4952, 0.9937, 1.2871],\n",
      "        [0.6542, 1.4554, 0.6929],\n",
      "        [1.8511, 1.5853, 0.8538],\n",
      "        [0.8609, 0.8204, 1.2031]])\n"
     ]
    }
   ],
   "source": [
    "# 加法形式1\n",
    "x = torch.rand(5, 3)\n",
    "y = torch.rand(5, 3)\n",
    "print(x + y)\n",
    "\n",
    "# 加法形式2\n",
    "print(torch.add(x, y))\n",
    "\n",
    "res = torch.empty(5, 3)\n",
    "print(torch.add(x, y, out=res))  # 指定输出\n",
    "\n",
    "# 加法形式3\n",
    "y.add_(x)  # 原地操作\n",
    "print(y)\n",
    "\n",
    "# ※ Pytorch 中所有的原地操作函数名都带 _"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-18T16:56:12.199665Z",
     "end_time": "2023-04-18T16:56:12.798101Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. 索引,是原值的引用,数据是共享内存的."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5365, 0.6096, 0.7764],\n",
      "        [0.9763, 0.0651, 0.9785],\n",
      "        [0.3924, 0.7445, 0.4088],\n",
      "        [0.9869, 0.8715, 0.1863],\n",
      "        [0.2719, 0.4114, 0.3044]])\n",
      "tensor([[0.9763, 0.0651, 0.9785],\n",
      "        [0.3924, 0.7445, 0.4088],\n",
      "        [0.9869, 0.8715, 0.1863],\n",
      "        [0.2719, 0.4114, 0.3044]])\n",
      "tensor([[0.0651, 0.9785],\n",
      "        [0.7445, 0.4088]])\n"
     ]
    }
   ],
   "source": [
    "x_sub = x[1:]\n",
    "x_sub_sub = x[1:3, 1:3]\n",
    "print(x)\n",
    "print(x_sub)\n",
    "print(x_sub_sub)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-18T16:56:12.230914Z",
     "end_time": "2023-04-18T16:56:12.844977Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5365, 0.6096, 0.7764],\n",
      "        [0.9763, 0.0651, 0.9785],\n",
      "        [0.3924, 0.7445, 0.4088],\n",
      "        [0.9869, 0.8715, 0.1863],\n",
      "        [0.2719, 0.4114, 0.3044]])\n",
      "tensor([[0.5365, 0.7764],\n",
      "        [0.9763, 0.9785],\n",
      "        [0.3924, 0.4088],\n",
      "        [0.9869, 0.1863],\n",
      "        [0.2719, 0.3044]])\n",
      "tensor([[0.5365, 0.6096, 0.7764],\n",
      "        [0.3924, 0.7445, 0.4088]])\n"
     ]
    }
   ],
   "source": [
    "# 索引的高级用法\n",
    "# index_select(input,dim,index) 可选择 input 数据上第 dim 个维度的 index\n",
    "ind = torch.tensor([0, 2])  # 要求取该维度上前两个元素\n",
    "a = torch.index_select(x, 1, ind)  # 取前两行\n",
    "b = torch.index_select(x, 0, ind)  # 取前两列\n",
    "\n",
    "print(x)\n",
    "print(a)\n",
    "print(b)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-18T16:56:12.305691Z",
     "end_time": "2023-04-18T16:56:12.844977Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-18T16:56:12.337000Z",
     "end_time": "2023-04-18T16:56:12.860603Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " 3. 使用 view() 改变 `Tensor`的 `shape`,但是同一份数据"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3]) torch.Size([15]) torch.Size([3, 5])\n",
      "原 x:\n",
      " tensor([[0.5365, 0.6096, 0.7764],\n",
      "        [0.9763, 0.0651, 0.9785],\n",
      "        [0.3924, 0.7445, 0.4088],\n",
      "        [0.9869, 0.8715, 0.1863],\n",
      "        [0.2719, 0.4114, 0.3044]])\n",
      "y*=2 后的 x:\n",
      " tensor([[1.0730, 1.2192, 1.5529],\n",
      "        [1.9525, 0.1301, 1.9571],\n",
      "        [0.7849, 1.4891, 0.8176],\n",
      "        [1.9738, 1.7430, 0.3725],\n",
      "        [0.5437, 0.8229, 0.6087]])\n"
     ]
    }
   ],
   "source": [
    "y = x.view(15)\n",
    "z = x.view(-1, 5)  # -1 表示按照其它维度自动推导\n",
    "print(x.size(), y.size(), z.size())\n",
    "\n",
    "print(\"原 x:\\n\", x)\n",
    "y *= 2\n",
    "print(\"y*=2 后的 x:\\n\", x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-18T16:56:12.368211Z",
     "end_time": "2023-04-18T16:56:12.860603Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原 x:\n",
      " tensor([[1.0730, 1.2192, 1.5529],\n",
      "        [1.9525, 0.1301, 1.9571],\n",
      "        [0.7849, 1.4891, 0.8176],\n",
      "        [1.9738, 1.7430, 0.3725],\n",
      "        [0.5437, 0.8229, 0.6087]])\n",
      "修改了 x_cp 后的x:\n",
      " tensor([[1.0730, 1.2192, 1.5529],\n",
      "        [1.9525, 0.1301, 1.9571],\n",
      "        [0.7849, 1.4891, 0.8176],\n",
      "        [1.9738, 1.7430, 0.3725],\n",
      "        [0.5437, 0.8229, 0.6087]])\n",
      "x_cp:\n",
      " tensor([-8.9270, -8.7808, -8.4471, -8.0475, -9.8699, -8.0429, -9.2151, -8.5109,\n",
      "        -9.1824, -8.0262, -8.2570, -9.6275, -9.4563, -9.1771, -9.3913])\n"
     ]
    }
   ],
   "source": [
    "# 链式依次调用 `clone().view()`,得到真正的一份副本\n",
    "x_cp = x.clone().view(15)\n",
    "\n",
    "print(\"原 x:\\n\", x)\n",
    "x_cp -= 10\n",
    "print(\"修改了 x_cp 后的x:\\n\", x)\n",
    "print(\"x_cp:\\n\", x_cp)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-18T16:56:12.415089Z",
     "end_time": "2023-04-18T16:56:12.860603Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "4. 转换为 Python 对象:`item()`将一个 `Tensor` 标量转换为一个 `Python Number`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.3099])\n",
      "-0.3099381923675537\n"
     ]
    }
   ],
   "source": [
    "n = torch.randn(1)\n",
    "print(n)\n",
    "print(n.item())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-18T16:56:12.461976Z",
     "end_time": "2023-04-18T16:56:12.860603Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "5. 堆叠 `Tensor`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [ 2.,  1.,  4.,  3.],\n",
      "        [ 1.,  2.,  3.,  4.],\n",
      "        [ 4.,  3.,  2.,  1.]])\n",
      "tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n",
      "        [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]])\n"
     ]
    }
   ],
   "source": [
    "sub_1 = torch.arange(12).reshape(3,4)\n",
    "sub_2 = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "\n",
    "print(torch.cat((sub_1, sub_2), dim=0))     # 按行堆叠\n",
    "print(torch.cat((sub_1, sub_2), dim=1))     # 按列堆叠"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T11:54:19.133787Z",
     "end_time": "2023-04-19T11:54:19.221780Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 三、广播机制\n",
    "前面的都是对同尺寸的 `Tensor` 按元素运算.\n",
    "当`Tensor`不同尺寸时,可能会触发**广播**机制:\n",
    "> ①适当复制元素使这两个 `Tensor` 形状相同\n",
    "> ②将这两个 `Tensor` 按元素运算"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2]])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "tensor([[2, 3],\n",
      "        [3, 4],\n",
      "        [4, 5]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(1, 3).view(1, 2)\n",
    "y = torch.arange(1, 4).view(3, 1)\n",
    "\n",
    "print(x)  # [1*2]\n",
    "print(y)  # [3*1]\n",
    "\n",
    "print(x + y)  # [3*2] + [3*2]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-18T16:56:12.477595Z",
     "end_time": "2023-04-18T16:56:12.860603Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 四、运算的内存开销\n",
    "\n",
    "如前所述,索引操作是**引用**,而`y = x + y`等算术操作是开辟新内存地址的,即左值`y`和右值`y`实际是不同的内存地址\n",
    "如果希望算术操作也能写入原地址，可以使用`[:]`操作"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same Memory:  True\n",
      "Same Memory:  True\n",
      "Same Memory:  True\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2])\n",
    "y = torch.tensor([3, 4])\n",
    "\n",
    "old_id = id(y)\n",
    "y[:] = y + x\n",
    "new_id = id(y)\n",
    "print(\"Same Memory: \", old_id == new_id)\n",
    "\n",
    "# 使用 `torch. add_(out=)` 也有相同效果\n",
    "old_id = id(y)\n",
    "torch.add(x, y, out=y)\n",
    "new_id = id(y)\n",
    "print(\"Same Memory: \", old_id == new_id)\n",
    "\n",
    "# 使用 `y.add_(x)` 也有相同效果\n",
    "old_id = id(y)\n",
    "y.add_(x)\n",
    "new_id = id(y)\n",
    "print(\"Same Memory: \", old_id == new_id)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-18T17:00:29.561904Z",
     "end_time": "2023-04-18T17:00:29.686911Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 五、`Tensor` 和 `NumPy` 相互转换"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(a): <class 'torch.Tensor'>\n",
      "type(na): <class 'numpy.ndarray'>\n",
      "[ 1.  1. 10.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "# Tensor -> NumPy : numpy()\n",
    "# NumPy -> Tensor : from_numpy()\n",
    "# 注意：两个函数所产生的的 Tensor 和 NumPy 中的数组共享相同的内存\n",
    "# 所有在 CPU 上的 Tensor（除了 CharTensor）都支持与 NumPy 数组相互转换。\n",
    "\n",
    "a = torch.ones(5)\n",
    "na = a.numpy()\n",
    "\n",
    "print(\"type(a):\",type(a))\n",
    "print(\"type(na):\",type(na))\n",
    "\n",
    "\n",
    "a[2] = 10\n",
    "print(na)  # [ 1.  1. 10.  1.  1.]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T12:02:18.555560Z",
     "end_time": "2023-04-19T12:02:18.715558Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4., 4., 4., 4., 4.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "nb = np.ones(5)\n",
    "b = torch.from_numpy(nb)\n",
    "nb += 3\n",
    "print(b)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-18T17:14:52.980910Z",
     "end_time": "2023-04-18T17:14:52.996585Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2.,  2., 11.,  2.,  2.]) tensor([ 1.,  1., 10.,  1.,  1.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\temp\\ipykernel_6264\\1074872290.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  c = torch.tensor(a)\n"
     ]
    }
   ],
   "source": [
    "# 复制，而非引用\n",
    "c = torch.tensor(a)\n",
    "a += 1\n",
    "print(a, c)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-18T17:16:26.028670Z",
     "end_time": "2023-04-18T17:16:26.200534Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 六、Tensor on GPU"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 以下代码只有在PyTorch GPU版本上才会执行\n",
    "if torch.cuda.is_available():\n",
    "    # A torch.device is an object representing the device on which a torch.Tensor is or will be allocated.\n",
    "    device = torch.device(\"cuda\")          # GPU\n",
    "    x=torch.arange(2<<25)\n",
    "    y = torch.randn(2<<25, device=device)  # 直接创建一个在GPU上的Tensor\n",
    "    x = x.to(device)                       # 等价于 .to(\"cuda\")\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))       # to()还可以同时更改数据类型"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-18T22:40:57.116248Z",
     "end_time": "2023-04-18T22:40:58.836409Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "'NVIDIA GeForce 940MX'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-11T03:16:31.804578800Z",
     "start_time": "2023-05-11T03:16:31.569161300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "['sm_37',\n 'sm_50',\n 'sm_60',\n 'sm_61',\n 'sm_70',\n 'sm_75',\n 'sm_80',\n 'sm_86',\n 'compute_37']"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_arch_list()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T11:28:14.456316Z",
     "end_time": "2023-04-19T11:28:14.833320Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "查看gpu信息： True\n",
      "GPU的数量： 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "#1.查看gpu信息\n",
    "print('查看gpu信息：', torch.cuda.is_available())\n",
    "\n",
    "#GPU的数量\n",
    "print('GPU的数量：', torch.cuda.device_count())\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T12:14:54.603522300Z",
     "start_time": "2023-07-15T12:14:54.451512Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "True\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "#2.将张量在gpu和cpu间移动\n",
    "tensor = torch.rand((100, 100))\n",
    "tensor_gpu = tensor.to('cuda:0') #或者tensor_gpu = tensor.cuda()\n",
    "print(tensor_gpu.device)\n",
    "print(tensor_gpu.is_cuda)\n",
    "\n",
    "tensor_cpu = tensor_gpu.to('cpu') #或者tensor_cpu = tensor_gpu.cpu()\n",
    "print(tensor_cpu.device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T12:15:25.219141800Z",
     "start_time": "2023-07-15T12:15:25.128147Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "#3.将模型中的全部张量移动到gpu上\n",
    "net = nn.Linear(2,1)\n",
    "print(next(net.parameters()).is_cuda)\n",
    "net.to('cuda:0')#将模型中的全部参数张量依次到GPU上，注意，无需重新赋值为net=net.to('cuda:0')\n",
    "print(next(net.parameters()).is_cuda)\n",
    "print(next(net.parameters()).device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T12:16:34.365982400Z",
     "start_time": "2023-07-15T12:16:34.177301700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "[0]\n",
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4.创建支持多个gpu数据并行的模型\n",
    "linear = nn.Linear(2,1)\n",
    "print(next(linear.parameters()).device)\n",
    "\n",
    "model = nn.DataParallel(linear)\n",
    "print(model.device_ids)\n",
    "print(next(model.module.parameters()).device)\n",
    "\n",
    "#注意保存参数时要指定保存model.module的参数\n",
    "torch.save(model.module.state_dict(), 'model_parameter.pkl')\n",
    "\n",
    "linear = nn.Linear(2,1)\n",
    "linear.load_state_dict(torch.load('model_parameter.pkl'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T12:26:59.481689200Z",
     "start_time": "2023-07-15T12:26:59.390727800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "#5.清空cuda缓存\n",
    "#该方在cuda超内存时十分有用\n",
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T12:27:11.338723900Z",
     "start_time": "2023-07-15T12:27:11.182691400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 补充： Pytorch 创建 Tensor 的四种方式的异同\n",
    "1. torch.Tensor(data)\n",
    "2. torch.tensor(data)\n",
    "3. torch.from_numpy(data)\n",
    "4. torch.as_tensor(data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输出的结果：\n",
      "tensor([1., 2., 3.])\n",
      "tensor([1, 2, 3], dtype=torch.int32)\n",
      "tensor([1, 2, 3], dtype=torch.int32)\n",
      "tensor([1, 2, 3], dtype=torch.int32)\n",
      "输出的类型：\n",
      "torch.float32\n",
      "torch.int32\n",
      "torch.int32\n",
      "torch.int32\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "data = np.array([1, 2, 3])\n",
    "\n",
    "Tensor = torch.Tensor(data)         # 构造函数，返回值默认为浮点类型       深拷贝\n",
    "tensor = torch.tensor(data)         # 普通函数，根据输入推断数据类型       深拷贝\n",
    "from_numpy = torch.from_numpy(data) # 普通函数，根据输入推断数据类型       浅拷贝\n",
    "as_tensor = torch.as_tensor(data)   # 普通函数，根据输入推断数据类型       浅拷贝\n",
    "\n",
    "print('输出的结果：')\n",
    "print(Tensor)\n",
    "print(tensor)\n",
    "print(from_numpy)\n",
    "print(as_tensor)\n",
    "\n",
    "print('输出的类型：')\n",
    "print(Tensor.dtype)\n",
    "print(tensor.dtype)\n",
    "print(from_numpy.dtype)\n",
    "print(as_tensor.dtype)\n",
    "\n",
    "# as_tensor() 更好\n",
    "# 因为 torch.as_tensor() 可以接受任何 Python 数据结构"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-11T04:09:45.774870800Z",
     "start_time": "2023-05-11T04:09:45.670873900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
